# -------------------------------------------------------------
# BCI-FeaST
# Copyright (c) 2024
#       Dirk Keller,
#       Elena Offenberg,
#       Nick Ramsey's Lab, University Medical Center Utrecht, University Utrecht
# Licensed under the MIT License [see LICENSE for detail]
# -------------------------------------------------------------
import multiprocessing
import random
from numbers import Integral, Real
from typing import Tuple, List, Union, Dict, Any, Optional, Callable, Type

import numpy
from deap import base, creator, tools
from deap.algorithms import eaSimple, eaMuCommaLambda, eaMuPlusLambda
from numba import njit
from sklearn.model_selection import BaseCrossValidator
from sklearn.pipeline import Pipeline
from sklearn.utils._param_validation import Interval, StrOptions
from tqdm import tqdm

from .Base_Optimizer import BaseOptimizer

__all__ = ["EvolutionaryAlgorithms"]

creator.create("FitnessMax", base.Fitness, weights=[1.0])
creator.create("Individual", numpy.ndarray, fitness=creator.FitnessMax)


class EvolutionaryAlgorithms(BaseOptimizer):
    """
    This class implements an evolutionary algorithm (EA) for optimizing the selection
    of feature combinations by iteratively improving a candidate solution according
    to a predefined measure of quality. It employs genetic algorithm techniques,
    such as crossover, mutation, and selection, provided by the DEAP library to
    evolve a population of potential solutions, aiming to find the best feature
    combinations for a specified metric. The class supports various evolutionary
    strategies (e.g., simple [1]_, MuPlusLambda, MuCommaLambda) and parallel island
    evolution. For further details, consult the DEAP documentation.

    The *simple* evolutionary algorithm optimizes a population of individuals over a
    series of (*ngen*) generations. At each generation, a selection process
    stochastically chooses individuals to entirely replace the parental population,
    allowing for repeated selection of the same individual. This selected population
    undergoes variation through crossover and mutation operations, producing
    offspring that form the next generation. Both crossover and mutation operations
    are applied in sequence and the probabilities of crossover (*cxpb*) and mutation
    (*mutpb*) determine the likelihood of each respective operation being applied to
    the individuals. After each variation step, the new individuals are evaluated for
    their fitness.

    The *MuPlusLambda* (:math:`(\mu + \lambda)`) and *MuCommaLambda* (:math:`(\mu,
    \lambda)`) evolutionary strategies differ from the simple evolutionary algorithm
    in their offspring generation and selection processes. Both strategies follow a
    model where, for each individual, one of the operations (crossover, mutation,
    or direct reproduction) is probabilistically applied based on the specified
    probabilities.

    The *MuPlusLambda* (:math:`(\mu + \lambda)`) strategy maintains a parent
    population of size *mu* and generates *lambda* offspring at each generation.
    Offspring are created using variation operators (crossover and mutation) applied
    to individuals selected from the parent population according to the probabilities
    *cxpb* (crossover) and *mutpb* (mutation). After evaluating the offspring for
    fitness, the next generation is formed by selecting the best *mu* individuals
    from the combined pool of parents and offspring. This approach allows for the
    survival of the fittest individuals from both the parent and offspring
    populations, promoting convergence while preserving genetic diversity.

    In contrast, the *MuCommaLambda* (:math:`(\mu, \lambda)`) strategy enforces a
    strict generational replacement policy. At each generation, *lambda* offspring
    are generated by applying variation operators (crossover and mutation) to the
    parent population, again governed by *cxpb* and *mutpb*. Only the offspring are
    considered for selection in the next generation; the parent population does not
    survive. The best *mu* individuals are selected solely from the offspring pool to
    form the new parent population. This strategy encourages exploration of the
    solution space by ensuring that each generation consists entirely of newly
    generated individuals, potentially enhancing diversity but also requiring careful
    management of the offspring-to-parent ratio to avoid premature convergence.

    Parameters:
    -----------
    :param dims: Tuple[int, ...]
        A tuple of dimensions indies tc apply the feature selection onto. Any
        combination of dimensions can be specified, except for dimension 'zero', which
        represents the samples.
    :param estimator: Union[Any, Pipeline]
        The machine learning model or pipeline to evaluate feature sets.
    :param estimator_params: Dict[str, any], optional
        Optional parameters to adjust the estimator parameters.
    :param scoring: str, default = 'f1_weighted'
        The metric to optimize. Must be scikit-learn compatible.
    :param cv: Union[BaseCrossValidator, int, float], default = 10
        The cross-validation strategy or number of folds. If an integer is passed,
        :code:`train_test_split` for 1 and :code:`StratifiedKFold` is used for >1 as
        default. A float below 1 represents the percentage of training samples for the
        train-test split ratio.
    :param groups: Optional[numpy.ndarray], optional
        Groups for a LeaveOneGroupOut generator.
    :param population_size: int, default = 120
        The size of the population in each generation.
    :param n_gen: int, default = 100
        The number of generations over which the population evolves.
    :param islands: int, default = 1
        The number of separate populations (islands) used in parallel evolutionary
        processes.
    :param offspring_generation: str, default = 'simple'
        Determines the method to generate the offspring. Valid options include 'simple',
         'mu_plus_lambda' and 'mu_lambda'.
    :param crossover: str, default = 'two_point'
        The method used for crossing over individuals in the population. Valid option
        are 'one_point', 'two_point','uniform','part_matched', 'uni_part_matched',
        'ordered', 'blend', 'es_two_point','sim_binary' and 'messy_one_point'.
    :param mutate: str, default = 'flip'
        The mutation method applied to offspring. Valid options include 'gaussian',
        'shuffle', 'flip' and 'es_log_normal'.
    :param selection: str, default = 'tournament'
        The method used to select individuals for the next generation. Valid options
        include 'tournament', 'roulette','nsga2','spea2', 'best', 'tournament_dcd',
        'stochastic_uni','lexicase','epsilon_lexicase' and 'auto_epsilon_lexicase'.
    :param mu: int, default = 30
        The number of individuals to select for the next generation in 'mu_plus_lambda'
        and 'mu_lambda' methods.
    :param lmbda: int, default = 60
        The number of children to produce in 'mu_plus_lambda' and 'mu_lambda' methods.
    :param migration_chance: float, default = 0.1
        The probability of migrating individuals among islands per generation.
    :param migration_size: int, default = 5
        The number of individuals to migrate between islands when migration occurs.
    :param cxpb: float, default = 0.5
        The probability of mating two individuals (crossover probability).
    :param mutpb: float, default = 0.2
        The probability of mutating an individual (mutation probability).
    :param cx_indpb: float, default = 0.05
        The independent probability of each attribute being exchanged during crossover.
    :param cx_alpha: float, default = 0.3
        The alpha value for blend crossover.
    :param cx_eta: float, default = 5
        The eta value for simulated binary crossover.
    :param mut_sigma: float, default = 0.1
        The standard deviation of the Gaussian distribution used for Gaussian mutation.
    :param mut_mu: float, default = 0
        The mean of the Gaussian distribution used for Gaussian mutation.
    :param mut_indpb: float, default = 0.2
        The independent probability of each attribute being mutated.
    :param mut_c: float, default = 0.01
        The c value for ES log-normal mutation.
    :param mut_eta: float, default = 40
        The eta value for polynomial mutation.
    :param sel_tournsize: int, default = 3
        The tournament size for tournament selection method.
    :param sel_nd: str, default = 'standard'
        The non-dominated sorting type for NSGA-II selection. Valid options are
        'standard' and 'log.
    :param patience: int, default = int(1e5)
        The number of iterations for which the objective function improvement must be
        below tol to stop optimization.
    :param tol: float, default = 1e-5
        The function tolerance; if the change in the best objective value is below this
        for 'patience' iterations, the optimization will stop early.
    :param bounds: Tuple[float, float], default = (0.0, 1.0)
        Bounds for the algorithm's parameters to optimize. Since it is a binary
        selection task, bounds are set to (0.0, 1.0).
    :param prior: numpy.ndarray, optional
        Explicitly initialize the optimizer state. If set to None, the to be optimized
        features are initialized randomly within the bounds.
    :param callback: Callable, optional
        A callback function of the structure :code: `callback(x, f, context)`, which
        will be called at each iteration. :code: `x` and :code: `f` are the solution and
        function value, and :code: `context` contains the diagnostics of the current
        iteration.
    :param n_jobs: int, default = 1
        The number of parallel jobs to run during cross-validation.
    :param random_state: int, optional
        Setting a seed to fix randomness (for reproducibility).
    :param verbose: Union[bool, int], default = False
         If set to True, enables the output of progress status during the optimization
         process.

    Methods:
    --------
    - fit:
        Fit the optimizer to the data.
    - transform:
        Transform the input data using the mask from the optimization process.

    Notes:
    ------
    This implementation is semi-compatible with the scikit-learn framework, which builds
    around two-dimensional feature matrices. To use this transformation within a
    scikit-learn Pipeline, the four dimensional data must be flattened after the first
    dimension [samples, features]. For example, scikit-learn's
    :code: `FunctionTransformer` can achieve this.

    Care must be taken when the lambda:mu ratio is 1 to 1 for the 'MuPlusLambda'
    and 'MuCommaLambda' algorithms as a non-stochastic selection will result in
    no selection at all as the operator selects lambda individuals from a pool
    of mu.

    References:
    -----------
    .. [1] Back, Fogel and Michalewicz, Evolutionary Computation 1 :Basic
        Algorithms and Operators, 2000.

    Examples:
    ---------
    The following example shows how to retrieve a feature mask for a synthetic data set.

    .. code-block:: python

        import numpy
        from sklearn.svm import SVC
        from sklearn.pipeline import Pipeline
        from sklearn.preprocessing import MinMaxScaler
        from sklearn.datasets import make_classification
        from FingersVsGestures.src.channel_elimination import EvolutionaryAlgorithms # TODO adjust

        X, y = make_classification(n_samples=100, n_features=8 * 4 * 100)
        X = X.reshape((100, 8, 4, 100))
        grid = (2, 3)
        estimator = Pipeline([('scaler', MinMaxScaler()), ('svc', SVC())])

        ea = ParticleSwarmOptimization(grid, estimator)
        ea.fit(X, y)
        print(ea.score_)
        26.545670995670996

    Returns:
    --------
    :return: None
    """

    # fmt: off
    _parameter_constraints: dict = {**BaseOptimizer._parameter_constraints}
    _parameter_constraints.update(
        {
            "population_size": [Interval(Integral, 1, None, closed="left")],
            "n_gen": [Interval(Integral, 1, None, closed="left")],
            "islands": [Interval(Integral, 1, None, closed="left")],
            "method": [StrOptions({"simple", "mu_plus_lambda", "mu_lambda"})],
            "crossover": [StrOptions({"one_point", "two_point", "uniform", "part_matched", "uni_part_matched",
                                      "ordered", "blend", "es_two_point", "sim_binary", "messy_one_point",})],
            "mutate": [StrOptions({"gaussian", "shuffle", "flip", "es_log_normal"})],
            "selection": [StrOptions({"tournament", "roulette", "nsga2", "spea2", "best", "tournament_dcd",
                                      "stochastic_uni", "lexicase", "epsilon_lexicase", "auto_epsilon_lexicase",})],
            "mu": [Interval(Integral, 1, None, closed="left")],
            "lmbda": [Interval(Integral, 1, None, closed="left")],
            "migration_chance": [Interval(Real, 0, 1, closed="both")],
            "migration_size": [Interval(Integral, 1, None, closed="left")],
            "cxpb": [Interval(Real, 0, 1, closed="both")],
            "mutpb": [Interval(Real, 0, 1, closed="both")],
            "cx_indpb": [Interval(Real, 0, 1, closed="both")],
            "cx_alpha": [Interval(Real, 0, 1, closed="both")],
            "cx_eta": [Interval(Real, 0, None, closed="left")],
            "mut_sigma": [Interval(Real, 0, None, closed="left")],
            "mut_mu": [Real],
            "mut_indpb": [Interval(Real, 0, 1, closed="both")],
            "mut_c": [Interval(Real, 0, None, closed="left")],
            "mut_eta": [Interval(Real, 0, None, closed="left")],
            "sel_tournsize": [Interval(Integral, 1, None, closed="left")],
            "sel_nd": [StrOptions({"standard", "log"})],
        }
    )
    # fmt: on

    def __init__(
        self,
        # General and Decoder
        dims: Tuple[int, ...],
        estimator: Union[Any, Pipeline],
        estimator_params: Optional[Dict[str, any]] = None,
        scoring: str = "f1_weighted",
        cv: Union[BaseCrossValidator, int, float] = 10,
        groups: Optional[numpy.ndarray] = None,
        # Genetic Algorithm Settings
        population_size: int = 120,
        n_gen: int = 100,
        islands: int = 1,
        offspring_generation: str = "simple",
        crossover: str = "two_point",
        mutate: str = "flip",
        selection: str = "tournament",
        mu: int = 30,
        lmbda: int = 60,
        # Crossover, Mutation, Selection adn Migration Parameters
        migration_chance: float = 0.1,
        migration_size: int = 5,
        cxpb: float = 0.5,
        mutpb: float = 0.2,
        cx_indpb: float = 0.05,
        cx_alpha: float = 0.3,
        cx_eta: float = 5,
        mut_sigma: float = 0.1,
        mut_mu: float = 0,
        mut_indpb: float = 0.2,
        mut_c: float = 0.01,
        mut_eta: float = 40,
        sel_tournsize: int = 3,
        sel_nd: str = "standard",
        # Training Settings
        tol: float = 1e-5,
        patience: int = int(1e5),
        bounds: Tuple[float, float] = (0.0, 1.0),
        prior: Optional[numpy.ndarray] = None,
        callback: Optional[Union[Callable, Type]] = None,
        # Misc
        n_jobs: int = 1,
        random_state: Optional[int] = None,
        verbose: Union[bool, int] = False,
    ) -> None:

        super().__init__(
            dims,
            estimator,
            estimator_params,
            scoring,
            cv,
            groups,
            tol,
            patience,
            bounds,
            prior,
            callback,
            n_jobs,
            random_state,
            verbose,
        )

        # Genetic Algorithm Settings
        self.population_size = population_size
        self.n_gen = n_gen
        self.islands = islands
        self.offspring_generation = offspring_generation
        self.crossover = crossover
        self.mutate = mutate
        self.selection = selection

        # Crossover, Mutation, Selection adn Migration Parameters
        self.migration_chance = migration_chance
        self.migration_size = migration_size
        self.cxpb = cxpb
        self.mutpb = mutpb
        self.mu = mu
        self.lmbda = lmbda
        self.cx_indpb = cx_indpb
        self.cx_alpha = cx_alpha
        self.cx_eta = cx_eta
        self.mut_sigma = mut_sigma
        self.mut_mu = mut_mu
        self.mut_indpb = mut_indpb
        self.mut_c = mut_c
        self.mut_eta = mut_eta
        self.sel_tournsize = sel_tournsize
        self.sel_nd = sel_nd

    def _run(self) -> Tuple[numpy.ndarray, numpy.ndarray, float]:
        """
        Executes the evolutionary algorithm to optimize the feature configuration, by
        evaluating the proposed candidate solutions in the objective function
        :code:`f` for a number of iterations :code:`n_iter.`

        Returns:
        --------
        :return Tuple[numpy.ndarray, numpy.ndarray, float]:
            The best found solution, mask, and their fitness score.
        """
        # Set up EA algorithm
        toolbox = self._init_toolbox()
        populations = self._initialize_population(toolbox)
        method, method_params = self._init_method()

        # Initialize history
        stats = tools.Statistics(lambda ind: ind.fitness.values)
        stats.register("avg", numpy.mean)
        stats.register("std", numpy.std)
        stats.register("min", numpy.min)
        stats.register("max", numpy.max)

        hof = tools.HallOfFame(
            self.population_size * self.islands, similar=numpy.array_equal
        )
        best_score, wait = 0.0, 0

        progress_bar = tqdm(
            range(self.n_gen),
            desc=self.__class__.__name__,
            postfix=f"{best_score:.6f}",
            disable=not self.verbose,
            leave=True,
        )

        # Run the search loop
        for self.iter_ in progress_bar:
            for i, island in enumerate(populations):
                # Optimize Feature Selection
                populations[i], log = method(
                    populations[i],
                    toolbox,
                    ngen=1,
                    stats=stats,
                    halloffame=hof,
                    verbose=False,
                    **method_params,
                )

            # Perform migration if applicable
            if self.islands > 1 and self.migration_chance >= random.random():
                populations = self._migrate(populations, topology="ring")

            # Update logs and early stopping
            score = max(log.select("max"))
            if best_score < score:
                best_score = hof.items[0].fitness.values[0]
                progress_bar.set_postfix(best_score=f"{best_score:.6f}")
                if abs(best_score - score) > self.tol:
                    wait = 0
            if wait > self.patience:
                progress_bar.set_postfix(
                    best_score=f"Early Stopping Criteria reached: {best_score:.6f}"
                )
                break
            elif score >= 1.0:
                progress_bar.set_postfix(
                    best_score=f"Maximum score reached: {best_score:.6f}"
                )
                break
            elif self.callback is not None:
                if self.callback(
                    best_score,
                    numpy.array(hof.items[0]).reshape(-1),
                    self.result_grid_,
                ):
                    progress_bar.set_postfix(
                        best_score=f"Stopped by callback: {best_score:.6f}"
                    )
                    break

        # Obtain the final best_cost and the final best_position
        best_solution = numpy.array(hof.items)
        best_state = numpy.array(hof.items[0]) > 0.5
        best_score = hof.items[0].fitness.values[0] * 100

        return best_solution, best_state, best_score

    def _initialize_population(self, toolbox: base.Toolbox) -> List[List[Any]]:
        """
        Initialize populations for each island with prior individuals.

        Parameters:
        -----------
        :param toolbox: base.Toolbox
            The toolbox object that describes the specified evolutionary process.

        Returns:
        --------
        :return: List[List[Any]]
            A population of several individuals.
        """
        if self.prior_ is not None:
            return self.prior_

        # Initialize populations for each island
        return [toolbox.population(n=self.population_size) for _ in range(self.islands)]

    def _migrate(
        self, islands: List[List[Any]], topology: str = "ring"
    ) -> List[List[Any]]:
        """
        Migrates individuals among islands based on a specified topology.

        Parameters:
        -----------
            :param islands: List[List[Any]]
                A list of populations (islands).
            :param topology: str, default = 'ring'
                The structure of the migration network.

        Returns:
        --------
        return: List[List[Any]]
            A list of populations (islands).
        """
        if topology == "ring":
            for i in range(self.islands):
                # Select k individuals from island i to migrate to island (i+1)
                emigrants = tools.selBest(islands[i], self.migration_size)
                islands[(i + 1) % self.islands].extend(emigrants)
                # Remove emigrants from original population
                for emigrant in emigrants:
                    islands[i].remove(emigrant)
        return islands

    def objective_function_wrapper(self, mask: numpy.ndarray) -> List[float]:
        """
        Wraps the objective function to adapt it for compatibility with the evolutionary
        algorithm framework.

        Parameters:
        -----------
        :params mask: numpy.ndarray
            A boolean array indicating which features are included in the subset.

        Returns:
        --------
        :return List[float]
            A list containing the objective function's score for the provided mask. Must
            have the same size as weights of the fitness function (e.g. size of one).
        """
        return [self._objective_function(mask)]

    def _handle_bounds(self) -> Tuple[float, float]:
        """
        Returns the bounds for the EA optimizer. If bounds are not set, default bounds
        of [0, 1] for each dimension are used.

        Returns:
        --------
        :return: Tuple[float, float]
            A tuple of two numpy arrays representing the lower and upper bounds.
        """
        return self.bounds

    def _handle_prior(self) -> Optional[List[Any]]:
        """
        This function checks the validity of the 'prior'; if a valid 'prior' is
        provided, it returns a list of islands containing a population of DEAP
        individuals either directly or generating one with Gaussian perturbations if
        ndarray (features, ) was provided.

        Raises:
        -------
        :raise ValueError:
            If 'prior' is not None and does not match any expected type or format.

        Returns:
        --------
        :return: List[Any], optional:
            A list of DEAP Individual objects generated from the 'prior' input.
             Otherwise, None if no 'prior' is provided.
        """
        if self.prior is None:
            return None

        # If list of islands of DEAP Individuals is provided
        if isinstance(self.prior, list) and isinstance(self.prior[0], list):
            if self.prior[0][0].size == (numpy.prod(self.dim_size_),) and hasattr(
                self.prior[0][0], "fitness"
            ):
                return self.prior

        # If 'prior' is a ndarray mask is provided
        if isinstance(self.prior, numpy.ndarray):
            gaus = lambda: abs(
                numpy.random.normal(loc=0, scale=0.06125, size=self.prior.size)
            )
            return [
                [
                    creator.Individual(
                        numpy.where(
                            self.prior.flatten() < 0.5, 0.49 - gaus(), 0.51 + gaus()
                        )
                    )
                    for _ in range(self.population_size)
                ]
                for _ in range(self.islands)
            ]

        raise ValueError(
            f"The argument 'prior' must be a weight matrix matching the "
            f"expected dimensions or a list of lists containing individuals "
            f"with a fitness attribute."
        )

    def _init_method(self) -> Tuple[Callable, Dict[str, Any]]:
        """
        Returns the evolutionary algorithm method and its parameters.

        Returns:
        --------
        :return: Tuple[Callable, Dict[str, Any]
            A tuple containing the method function and a dictionary of its parameters.
        """
        # fmt: off
        return {
            "simple": (eaSimple, {"cxpb": self.cxpb, "mutpb": self.mutpb}),
            "mu_plus_lambda": (eaMuPlusLambda, {"cxpb": self.cxpb, "mutpb": self.mutpb, "mu": self.mu, "lambda_": self.lmbda}),
            "mu_lambda": (eaMuCommaLambda, {"cxpb": self.cxpb, "mutpb": self.mutpb, "mu": self.mu, "lambda_": self.lmbda}),
        }[self.offspring_generation]
        # fmt: on

    def _init_crossover(
        self,
    ) -> Tuple[Callable, Dict[str, Any]]:
        """
        Returns the crossover function and its parameters.

        Returns:
        --------
        :return: Tuple[Callable, Dict[str, Any]
            A tuple containing the crossover function and a dictionary of its
            parameters.
        """
        # fmt: off
        return {
            "one_point": (tools.cxOnePoint, {}),
            "two_point": (tools.cxTwoPoint, {}),
            "uniform": (tools.cxUniform, {"indpb": self.cx_indpb}),
            "part_matched": (tools.cxPartialyMatched, {}),
            "uni_part_matched": (tools.cxUniformPartialyMatched, {}),
            "ordered": (tools.cxOrdered, {}),
            "blend": (tools.cxBlend, {"alpha": self.cx_alpha}),
            "es_two_point": (tools.cxESTwoPoint, {"alpha": self.cx_alpha}),
            "sim_binary": (tools.cxSimulatedBinary, {"eta": self.cx_eta}),
            "messy_one_point": (tools.cxMessyOnePoint, {}),
        }[self.crossover]

    # fmt: on

    def _init_mutation(
        self,
    ) -> Tuple[Callable, Dict[str, Any]]:
        """
        Returns the mutation function and its parameters.

        Returns:
        --------
        :return: Tuple[Callable, Dict[str, Any]
            A tuple containing the mutation function and a dictionary of its parameters.
        """
        # fmt: off
        return {
            "gaussian": (tools.mutGaussian, {"mu": self.mut_mu, "sigma": self.mut_sigma, "indpb": self.mut_indpb}),
            "shuffle": (tools.mutShuffleIndexes, {"indpb": self.mut_indpb}),
            "flip": (tools.mutFlipBit, {"indpb": self.mut_indpb}),
            "es_log_normal": (tools.mutESLogNormal, {"c": self.mut_c, "indpb": self.mut_indpb}),
        }[self.mutate]

    # fmt: on

    def _init_selection(
        self,
    ) -> Tuple[Callable, Dict[str, Any]]:
        """
        Returns the selection function and its parameters.

        Returns:
        --------
        :return: Tuple[Callable, Dict[str, Any]
            A tuple containing the selection function and a dictionary of its
            parameters.
        """
        # fmt: off
        return {
            "tournament": (tools.selTournament, {"tournsize": self.sel_tournsize}),
            "roulette": (tools.selRoulette, {}),
            "nsga2": (tools.selNSGA2, {"nd": self.sel_nd}),
            "spea2": (tools.selSPEA2, {}),
            "best": (tools.selBest, {}),
            "tournament_dcd": (tools.selTournamentDCD, {"nd": self.sel_nd}),
            "stochastic_uni": (tools.selStochasticUniversalSampling, {}),
            "lexicase": (tools.selLexicase, {}),
            "epsilon_lexicase": (tools.selEpsilonLexicase, {}),
            "auto_epsilon_lexicase": (tools.selAutomaticEpsilonLexicase, {}),
        }[self.selection]

    # fmt: on

    def _init_toolbox(self) -> base.Toolbox:
        """
        Initializes the DEAP toolbox with fitness, individual, and population
        registration, as well as the genetic operators: crossover, mutation,
        and selection.

        Returns:
        --------
        :return: base.Toolbox
            The toolbox object that describes the specified evolutionary process.
        """

        # Step 1: Initialize the toolbox
        toolbox = base.Toolbox()

        if self.n_jobs > 1:
            pool = multiprocessing.Pool()
            toolbox.register("map", pool.map)

        def create_individual(n):
            """Create an individual using NumPy array and assign fitness."""
            return creator.Individual(
                numpy.random.uniform(self.bounds_[0], self.bounds_[1], n)
            )

        # Step 2: Register the attribute, individual, and population creation functions
        toolbox.register("individual", create_individual, numpy.prod(self.dim_size_))
        toolbox.register("population", tools.initRepeat, list, toolbox.individual)

        # Step 3: Register the evaluation, crossover, mutation, and selection functions
        toolbox.register("evaluate", self.objective_function_wrapper)
        crossover, crossover_params = self._init_crossover()
        toolbox.register("mate", crossover, **crossover_params)
        mutation, mutation_params = self._init_mutation()
        toolbox.register("mutate", mutation, **mutation_params)
        selection, selection_params = self._init_selection()
        toolbox.register("select", selection, **selection_params)

        @njit
        def clip_individual(individual, lower_bound, upper_bound):
            """Clip the individual's values within the specified bounds."""
            numpy.clip(individual, lower_bound, upper_bound, out=individual)
            return individual

        # fmt: off
        # Step 4: Decorate the mate and mutate functions with clipping
        toolbox.decorate("mate",lambda func: lambda ind1, ind2, **kwargs:
        (
            clip_individual(func(ind1, ind2, **kwargs)[0], self.bounds_[0], self.bounds_[1]),
            clip_individual(func(ind1, ind2, **kwargs)[1], self.bounds_[0], self.bounds_[1]),
        ),
                         )
        toolbox.decorate("mutate",lambda func: lambda ind, **kwargs:
        (
            clip_individual(func(ind, **kwargs)[0], self.bounds_[0], self.bounds_[1]),
        ),
                         )
        # fmt: on
        return toolbox
